{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd69feb-214b-456a-80b3-2f2bf54f92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d7becc-4285-4b15-b181-8904f4994fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20000, Training Loss: 6.061600, Validation Loss: 5.950627\n",
      "Epoch 2/20000, Training Loss: 5.951417, Validation Loss: 5.841536\n",
      "Epoch 3/20000, Training Loss: 5.842445, Validation Loss: 5.733677\n",
      "Epoch 4/20000, Training Loss: 5.734697, Validation Loss: 5.627065\n",
      "Epoch 5/20000, Training Loss: 5.628195, Validation Loss: 5.521712\n",
      "Epoch 6/20000, Training Loss: 5.522947, Validation Loss: 5.417631\n",
      "Epoch 7/20000, Training Loss: 5.418969, Validation Loss: 5.314833\n",
      "Epoch 8/20000, Training Loss: 5.316270, Validation Loss: 5.213325\n",
      "Epoch 9/20000, Training Loss: 5.214858, Validation Loss: 5.113114\n",
      "Epoch 10/20000, Training Loss: 5.114741, Validation Loss: 5.014210\n",
      "Epoch 11/20000, Training Loss: 5.015927, Validation Loss: 4.916619\n",
      "Epoch 12/20000, Training Loss: 4.918422, Validation Loss: 4.820345\n",
      "Epoch 13/20000, Training Loss: 4.822231, Validation Loss: 4.725395\n",
      "Epoch 14/20000, Training Loss: 4.727359, Validation Loss: 4.631769\n",
      "Epoch 15/20000, Training Loss: 4.633809, Validation Loss: 4.539471\n",
      "Epoch 16/20000, Training Loss: 4.541581, Validation Loss: 4.448499\n",
      "Epoch 17/20000, Training Loss: 4.450676, Validation Loss: 4.358855\n",
      "Epoch 18/20000, Training Loss: 4.361095, Validation Loss: 4.270536\n",
      "Epoch 19/20000, Training Loss: 4.272838, Validation Loss: 4.183541\n",
      "Epoch 20/20000, Training Loss: 4.185901, Validation Loss: 4.097866\n",
      "Epoch 21/20000, Training Loss: 4.100283, Validation Loss: 4.013509\n",
      "Epoch 22/20000, Training Loss: 4.015980, Validation Loss: 3.930466\n",
      "Epoch 23/20000, Training Loss: 3.932991, Validation Loss: 3.848733\n",
      "Epoch 24/20000, Training Loss: 3.851310, Validation Loss: 3.768306\n",
      "Epoch 25/20000, Training Loss: 3.770933, Validation Loss: 3.689179\n",
      "Epoch 26/20000, Training Loss: 3.691853, Validation Loss: 3.611347\n",
      "Epoch 27/20000, Training Loss: 3.614064, Validation Loss: 3.534802\n",
      "Epoch 28/20000, Training Loss: 3.537561, Validation Loss: 3.459538\n",
      "Epoch 29/20000, Training Loss: 3.462336, Validation Loss: 3.385544\n",
      "Epoch 30/20000, Training Loss: 3.388378, Validation Loss: 3.312813\n",
      "Epoch 31/20000, Training Loss: 3.315682, Validation Loss: 3.241336\n",
      "Epoch 32/20000, Training Loss: 3.244237, Validation Loss: 3.171103\n",
      "Epoch 33/20000, Training Loss: 3.174034, Validation Loss: 3.102104\n",
      "Epoch 34/20000, Training Loss: 3.105064, Validation Loss: 3.034329\n",
      "Epoch 35/20000, Training Loss: 3.037314, Validation Loss: 2.967767\n",
      "Epoch 36/20000, Training Loss: 2.970775, Validation Loss: 2.902406\n",
      "Epoch 37/20000, Training Loss: 2.905436, Validation Loss: 2.838234\n",
      "Epoch 38/20000, Training Loss: 2.841285, Validation Loss: 2.775239\n",
      "Epoch 39/20000, Training Loss: 2.778310, Validation Loss: 2.713410\n",
      "Epoch 40/20000, Training Loss: 2.716498, Validation Loss: 2.652733\n",
      "Epoch 41/20000, Training Loss: 2.655837, Validation Loss: 2.593195\n",
      "Epoch 42/20000, Training Loss: 2.596314, Validation Loss: 2.534785\n",
      "Epoch 43/20000, Training Loss: 2.537915, Validation Loss: 2.477489\n",
      "Epoch 44/20000, Training Loss: 2.480628, Validation Loss: 2.421293\n",
      "Epoch 45/20000, Training Loss: 2.424441, Validation Loss: 2.366183\n",
      "Epoch 46/20000, Training Loss: 2.369338, Validation Loss: 2.312146\n",
      "Epoch 47/20000, Training Loss: 2.315306, Validation Loss: 2.259168\n",
      "Epoch 48/20000, Training Loss: 2.262332, Validation Loss: 2.207236\n",
      "Epoch 49/20000, Training Loss: 2.210402, Validation Loss: 2.156335\n",
      "Epoch 50/20000, Training Loss: 2.159502, Validation Loss: 2.106452\n",
      "Epoch 51/20000, Training Loss: 2.109617, Validation Loss: 2.057572\n",
      "Epoch 52/20000, Training Loss: 2.060735, Validation Loss: 2.009682\n",
      "Epoch 53/20000, Training Loss: 2.012840, Validation Loss: 1.962767\n",
      "Epoch 54/20000, Training Loss: 1.965921, Validation Loss: 1.916812\n",
      "Epoch 55/20000, Training Loss: 1.919962, Validation Loss: 1.871806\n",
      "Epoch 56/20000, Training Loss: 1.874949, Validation Loss: 1.827733\n",
      "Epoch 57/20000, Training Loss: 1.830869, Validation Loss: 1.784579\n",
      "Epoch 58/20000, Training Loss: 1.787707, Validation Loss: 1.742332\n",
      "Epoch 59/20000, Training Loss: 1.745451, Validation Loss: 1.700977\n",
      "Epoch 60/20000, Training Loss: 1.704086, Validation Loss: 1.660501\n",
      "Epoch 61/20000, Training Loss: 1.663599, Validation Loss: 1.620890\n",
      "Epoch 62/20000, Training Loss: 1.623976, Validation Loss: 1.582129\n",
      "Epoch 63/20000, Training Loss: 1.585202, Validation Loss: 1.544207\n",
      "Epoch 64/20000, Training Loss: 1.547267, Validation Loss: 1.507109\n",
      "Epoch 65/20000, Training Loss: 1.510154, Validation Loss: 1.470822\n",
      "Epoch 66/20000, Training Loss: 1.473852, Validation Loss: 1.435333\n",
      "Epoch 67/20000, Training Loss: 1.438347, Validation Loss: 1.400628\n",
      "Epoch 68/20000, Training Loss: 1.403626, Validation Loss: 1.366696\n",
      "Epoch 69/20000, Training Loss: 1.369676, Validation Loss: 1.333522\n",
      "Epoch 70/20000, Training Loss: 1.336485, Validation Loss: 1.301095\n",
      "Epoch 71/20000, Training Loss: 1.304039, Validation Loss: 1.269401\n",
      "Epoch 72/20000, Training Loss: 1.272326, Validation Loss: 1.238428\n",
      "Epoch 73/20000, Training Loss: 1.241334, Validation Loss: 1.208164\n",
      "Epoch 74/20000, Training Loss: 1.211050, Validation Loss: 1.178596\n",
      "Epoch 75/20000, Training Loss: 1.181462, Validation Loss: 1.149712\n",
      "Epoch 76/20000, Training Loss: 1.152557, Validation Loss: 1.121500\n",
      "Epoch 77/20000, Training Loss: 1.124324, Validation Loss: 1.093948\n",
      "Epoch 78/20000, Training Loss: 1.096750, Validation Loss: 1.067044\n",
      "Epoch 79/20000, Training Loss: 1.069825, Validation Loss: 1.040777\n",
      "Epoch 80/20000, Training Loss: 1.043535, Validation Loss: 1.015134\n",
      "Epoch 81/20000, Training Loss: 1.017869, Validation Loss: 0.990104\n",
      "Epoch 82/20000, Training Loss: 0.992817, Validation Loss: 0.965676\n",
      "Epoch 83/20000, Training Loss: 0.968365, Validation Loss: 0.941838\n",
      "Epoch 84/20000, Training Loss: 0.944504, Validation Loss: 0.918580\n",
      "Epoch 85/20000, Training Loss: 0.921223, Validation Loss: 0.895891\n",
      "Epoch 86/20000, Training Loss: 0.898508, Validation Loss: 0.873759\n",
      "Epoch 87/20000, Training Loss: 0.876352, Validation Loss: 0.852174\n",
      "Epoch 88/20000, Training Loss: 0.854741, Validation Loss: 0.831125\n",
      "Epoch 89/20000, Training Loss: 0.833667, Validation Loss: 0.810602\n",
      "Epoch 90/20000, Training Loss: 0.813118, Validation Loss: 0.790594\n",
      "Epoch 91/20000, Training Loss: 0.793083, Validation Loss: 0.771091\n",
      "Epoch 92/20000, Training Loss: 0.773554, Validation Loss: 0.752083\n",
      "Epoch 93/20000, Training Loss: 0.754519, Validation Loss: 0.733559\n",
      "Epoch 94/20000, Training Loss: 0.735969, Validation Loss: 0.715510\n",
      "Epoch 95/20000, Training Loss: 0.717893, Validation Loss: 0.697926\n",
      "Epoch 96/20000, Training Loss: 0.700283, Validation Loss: 0.680798\n",
      "Epoch 97/20000, Training Loss: 0.683128, Validation Loss: 0.664115\n",
      "Epoch 98/20000, Training Loss: 0.666420, Validation Loss: 0.647870\n",
      "Epoch 99/20000, Training Loss: 0.650147, Validation Loss: 0.632051\n",
      "Epoch 100/20000, Training Loss: 0.634302, Validation Loss: 0.616650\n",
      "Epoch 101/20000, Training Loss: 0.618875, Validation Loss: 0.601658\n",
      "Epoch 102/20000, Training Loss: 0.603858, Validation Loss: 0.587066\n",
      "Epoch 103/20000, Training Loss: 0.589240, Validation Loss: 0.572865\n",
      "Epoch 104/20000, Training Loss: 0.575014, Validation Loss: 0.559048\n",
      "Epoch 105/20000, Training Loss: 0.561170, Validation Loss: 0.545604\n",
      "Epoch 106/20000, Training Loss: 0.547701, Validation Loss: 0.532526\n",
      "Epoch 107/20000, Training Loss: 0.534598, Validation Loss: 0.519806\n",
      "Epoch 108/20000, Training Loss: 0.521852, Validation Loss: 0.507436\n",
      "Epoch 109/20000, Training Loss: 0.509455, Validation Loss: 0.495406\n",
      "Epoch 110/20000, Training Loss: 0.497400, Validation Loss: 0.483710\n",
      "Epoch 111/20000, Training Loss: 0.485679, Validation Loss: 0.472339\n",
      "Epoch 112/20000, Training Loss: 0.474283, Validation Loss: 0.461286\n",
      "Epoch 113/20000, Training Loss: 0.463205, Validation Loss: 0.450543\n",
      "Epoch 114/20000, Training Loss: 0.452437, Validation Loss: 0.440103\n",
      "Epoch 115/20000, Training Loss: 0.441972, Validation Loss: 0.429959\n",
      "Epoch 116/20000, Training Loss: 0.431803, Validation Loss: 0.420103\n",
      "Epoch 117/20000, Training Loss: 0.421923, Validation Loss: 0.410528\n",
      "Epoch 118/20000, Training Loss: 0.412324, Validation Loss: 0.401228\n",
      "Epoch 119/20000, Training Loss: 0.403000, Validation Loss: 0.392195\n",
      "Epoch 120/20000, Training Loss: 0.393943, Validation Loss: 0.383423\n",
      "Epoch 121/20000, Training Loss: 0.385147, Validation Loss: 0.374905\n",
      "Epoch 122/20000, Training Loss: 0.376606, Validation Loss: 0.366635\n",
      "Epoch 123/20000, Training Loss: 0.368312, Validation Loss: 0.358606\n",
      "Epoch 124/20000, Training Loss: 0.360260, Validation Loss: 0.350812\n",
      "Epoch 125/20000, Training Loss: 0.352444, Validation Loss: 0.343247\n",
      "Epoch 126/20000, Training Loss: 0.344857, Validation Loss: 0.335905\n",
      "Epoch 127/20000, Training Loss: 0.337493, Validation Loss: 0.328781\n",
      "Epoch 128/20000, Training Loss: 0.330346, Validation Loss: 0.321867\n",
      "Epoch 129/20000, Training Loss: 0.323411, Validation Loss: 0.315159\n",
      "Epoch 130/20000, Training Loss: 0.316681, Validation Loss: 0.308651\n",
      "Epoch 131/20000, Training Loss: 0.310153, Validation Loss: 0.302338\n",
      "Epoch 132/20000, Training Loss: 0.303818, Validation Loss: 0.296214\n",
      "Epoch 133/20000, Training Loss: 0.297674, Validation Loss: 0.290274\n",
      "Epoch 134/20000, Training Loss: 0.291714, Validation Loss: 0.284513\n",
      "Epoch 135/20000, Training Loss: 0.285933, Validation Loss: 0.278926\n",
      "Epoch 136/20000, Training Loss: 0.280326, Validation Loss: 0.273508\n",
      "Epoch 137/20000, Training Loss: 0.274888, Validation Loss: 0.268254\n",
      "Epoch 138/20000, Training Loss: 0.269615, Validation Loss: 0.263160\n",
      "Epoch 139/20000, Training Loss: 0.264501, Validation Loss: 0.258221\n",
      "Epoch 140/20000, Training Loss: 0.259543, Validation Loss: 0.253432\n",
      "Epoch 141/20000, Training Loss: 0.254736, Validation Loss: 0.248789\n",
      "Epoch 142/20000, Training Loss: 0.250075, Validation Loss: 0.244288\n",
      "Epoch 143/20000, Training Loss: 0.245557, Validation Loss: 0.239925\n",
      "Epoch 144/20000, Training Loss: 0.241176, Validation Loss: 0.235695\n",
      "Epoch 145/20000, Training Loss: 0.236929, Validation Loss: 0.231595\n",
      "Epoch 146/20000, Training Loss: 0.232812, Validation Loss: 0.227621\n",
      "Epoch 147/20000, Training Loss: 0.228821, Validation Loss: 0.223768\n",
      "Epoch 148/20000, Training Loss: 0.224952, Validation Loss: 0.220033\n",
      "Epoch 149/20000, Training Loss: 0.221202, Validation Loss: 0.216413\n",
      "Epoch 150/20000, Training Loss: 0.217566, Validation Loss: 0.212904\n",
      "Epoch 151/20000, Training Loss: 0.214042, Validation Loss: 0.209502\n",
      "Epoch 152/20000, Training Loss: 0.210625, Validation Loss: 0.206205\n",
      "Epoch 153/20000, Training Loss: 0.207313, Validation Loss: 0.203008\n",
      "Epoch 154/20000, Training Loss: 0.204102, Validation Loss: 0.199910\n",
      "Epoch 155/20000, Training Loss: 0.200989, Validation Loss: 0.196905\n",
      "Epoch 156/20000, Training Loss: 0.197972, Validation Loss: 0.193993\n",
      "Epoch 157/20000, Training Loss: 0.195046, Validation Loss: 0.191169\n",
      "Epoch 158/20000, Training Loss: 0.192208, Validation Loss: 0.188431\n",
      "Epoch 159/20000, Training Loss: 0.189458, Validation Loss: 0.185776\n",
      "Epoch 160/20000, Training Loss: 0.186790, Validation Loss: 0.183201\n",
      "Epoch 161/20000, Training Loss: 0.184203, Validation Loss: 0.180704\n",
      "Epoch 162/20000, Training Loss: 0.181693, Validation Loss: 0.178282\n",
      "Epoch 163/20000, Training Loss: 0.179260, Validation Loss: 0.175933\n",
      "Epoch 164/20000, Training Loss: 0.176899, Validation Loss: 0.173654\n",
      "Epoch 165/20000, Training Loss: 0.174608, Validation Loss: 0.171443\n",
      "Epoch 166/20000, Training Loss: 0.172386, Validation Loss: 0.169297\n",
      "Epoch 167/20000, Training Loss: 0.170229, Validation Loss: 0.167215\n",
      "Epoch 168/20000, Training Loss: 0.168136, Validation Loss: 0.165194\n",
      "Epoch 169/20000, Training Loss: 0.166105, Validation Loss: 0.163233\n",
      "Epoch 170/20000, Training Loss: 0.164133, Validation Loss: 0.161328\n",
      "Epoch 171/20000, Training Loss: 0.162218, Validation Loss: 0.159479\n",
      "Epoch 172/20000, Training Loss: 0.160359, Validation Loss: 0.157683\n",
      "Epoch 173/20000, Training Loss: 0.158553, Validation Loss: 0.155939\n",
      "Epoch 174/20000, Training Loss: 0.156799, Validation Loss: 0.154244\n",
      "Epoch 175/20000, Training Loss: 0.155095, Validation Loss: 0.152596\n",
      "Epoch 176/20000, Training Loss: 0.153438, Validation Loss: 0.150996\n",
      "Epoch 177/20000, Training Loss: 0.151829, Validation Loss: 0.149440\n",
      "Epoch 178/20000, Training Loss: 0.150264, Validation Loss: 0.147926\n",
      "Epoch 179/20000, Training Loss: 0.148742, Validation Loss: 0.146455\n",
      "Epoch 180/20000, Training Loss: 0.147262, Validation Loss: 0.145023\n",
      "Epoch 181/20000, Training Loss: 0.145822, Validation Loss: 0.143631\n",
      "Epoch 182/20000, Training Loss: 0.144421, Validation Loss: 0.142275\n",
      "Epoch 183/20000, Training Loss: 0.143057, Validation Loss: 0.140955\n",
      "Epoch 184/20000, Training Loss: 0.141730, Validation Loss: 0.139670\n",
      "Epoch 185/20000, Training Loss: 0.140437, Validation Loss: 0.138419\n",
      "Epoch 186/20000, Training Loss: 0.139178, Validation Loss: 0.137199\n",
      "Epoch 187/20000, Training Loss: 0.137952, Validation Loss: 0.136011\n",
      "Epoch 188/20000, Training Loss: 0.136756, Validation Loss: 0.134852\n",
      "Epoch 189/20000, Training Loss: 0.135591, Validation Loss: 0.133723\n",
      "Epoch 190/20000, Training Loss: 0.134454, Validation Loss: 0.132621\n",
      "Epoch 191/20000, Training Loss: 0.133346, Validation Loss: 0.131546\n",
      "Epoch 192/20000, Training Loss: 0.132264, Validation Loss: 0.130497\n",
      "Epoch 193/20000, Training Loss: 0.131208, Validation Loss: 0.129472\n",
      "Epoch 194/20000, Training Loss: 0.130178, Validation Loss: 0.128472\n",
      "Epoch 195/20000, Training Loss: 0.129171, Validation Loss: 0.127495\n",
      "Epoch 196/20000, Training Loss: 0.128188, Validation Loss: 0.126539\n",
      "Epoch 197/20000, Training Loss: 0.127227, Validation Loss: 0.125606\n",
      "Epoch 198/20000, Training Loss: 0.126288, Validation Loss: 0.124693\n",
      "Epoch 199/20000, Training Loss: 0.125370, Validation Loss: 0.123800\n",
      "Epoch 200/20000, Training Loss: 0.124471, Validation Loss: 0.122926\n",
      "Epoch 201/20000, Training Loss: 0.123592, Validation Loss: 0.122070\n",
      "Epoch 202/20000, Training Loss: 0.122732, Validation Loss: 0.121233\n",
      "Epoch 203/20000, Training Loss: 0.121889, Validation Loss: 0.120412\n",
      "Epoch 204/20000, Training Loss: 0.121064, Validation Loss: 0.119608\n",
      "Epoch 205/20000, Training Loss: 0.120255, Validation Loss: 0.118820\n",
      "Epoch 206/20000, Training Loss: 0.119463, Validation Loss: 0.118047\n",
      "Epoch 207/20000, Training Loss: 0.118686, Validation Loss: 0.117290\n",
      "Epoch 208/20000, Training Loss: 0.117924, Validation Loss: 0.116546\n",
      "Epoch 209/20000, Training Loss: 0.117176, Validation Loss: 0.115816\n",
      "Epoch 210/20000, Training Loss: 0.116442, Validation Loss: 0.115099\n",
      "Epoch 211/20000, Training Loss: 0.115722, Validation Loss: 0.114396\n",
      "Epoch 212/20000, Training Loss: 0.115014, Validation Loss: 0.113704\n",
      "Epoch 213/20000, Training Loss: 0.114318, Validation Loss: 0.113024\n",
      "Epoch 214/20000, Training Loss: 0.113635, Validation Loss: 0.112356\n",
      "Epoch 215/20000, Training Loss: 0.112963, Validation Loss: 0.111699\n",
      "Epoch 216/20000, Training Loss: 0.112302, Validation Loss: 0.111052\n",
      "Epoch 217/20000, Training Loss: 0.111652, Validation Loss: 0.110416\n",
      "Epoch 218/20000, Training Loss: 0.111012, Validation Loss: 0.109790\n",
      "Epoch 219/20000, Training Loss: 0.110382, Validation Loss: 0.109173\n",
      "Epoch 220/20000, Training Loss: 0.109762, Validation Loss: 0.108566\n",
      "Epoch 221/20000, Training Loss: 0.109151, Validation Loss: 0.107967\n",
      "Epoch 222/20000, Training Loss: 0.108549, Validation Loss: 0.107378\n",
      "Epoch 223/20000, Training Loss: 0.107956, Validation Loss: 0.106796\n",
      "Epoch 224/20000, Training Loss: 0.107371, Validation Loss: 0.106223\n",
      "Epoch 225/20000, Training Loss: 0.106794, Validation Loss: 0.105657\n",
      "Epoch 226/20000, Training Loss: 0.106225, Validation Loss: 0.105099\n",
      "Epoch 227/20000, Training Loss: 0.105664, Validation Loss: 0.104548\n",
      "Epoch 228/20000, Training Loss: 0.105110, Validation Loss: 0.104004\n",
      "Epoch 229/20000, Training Loss: 0.104562, Validation Loss: 0.103467\n",
      "Epoch 230/20000, Training Loss: 0.104022, Validation Loss: 0.102936\n",
      "Epoch 231/20000, Training Loss: 0.103488, Validation Loss: 0.102412\n",
      "Epoch 232/20000, Training Loss: 0.102961, Validation Loss: 0.101894\n",
      "Epoch 233/20000, Training Loss: 0.102440, Validation Loss: 0.101382\n",
      "Epoch 234/20000, Training Loss: 0.101925, Validation Loss: 0.100875\n",
      "Epoch 235/20000, Training Loss: 0.101415, Validation Loss: 0.100375\n",
      "Epoch 236/20000, Training Loss: 0.100911, Validation Loss: 0.099879\n",
      "Epoch 237/20000, Training Loss: 0.100413, Validation Loss: 0.099389\n",
      "Epoch 238/20000, Training Loss: 0.099920, Validation Loss: 0.098904\n",
      "Epoch 239/20000, Training Loss: 0.099432, Validation Loss: 0.098424\n",
      "Epoch 240/20000, Training Loss: 0.098949, Validation Loss: 0.097948\n",
      "Epoch 241/20000, Training Loss: 0.098471, Validation Loss: 0.097478\n",
      "Epoch 242/20000, Training Loss: 0.097997, Validation Loss: 0.097011\n",
      "Epoch 243/20000, Training Loss: 0.097528, Validation Loss: 0.096550\n",
      "Epoch 244/20000, Training Loss: 0.097064, Validation Loss: 0.096092\n",
      "Epoch 245/20000, Training Loss: 0.096603, Validation Loss: 0.095639\n",
      "Epoch 246/20000, Training Loss: 0.096147, Validation Loss: 0.095190\n",
      "Epoch 247/20000, Training Loss: 0.095695, Validation Loss: 0.094744\n",
      "Epoch 248/20000, Training Loss: 0.095247, Validation Loss: 0.094303\n",
      "Epoch 249/20000, Training Loss: 0.094803, Validation Loss: 0.093865\n",
      "Epoch 250/20000, Training Loss: 0.094362, Validation Loss: 0.093431\n",
      "Epoch 251/20000, Training Loss: 0.093926, Validation Loss: 0.093000\n",
      "Epoch 252/20000, Training Loss: 0.093492, Validation Loss: 0.092573\n",
      "Epoch 253/20000, Training Loss: 0.093062, Validation Loss: 0.092149\n",
      "Epoch 254/20000, Training Loss: 0.092636, Validation Loss: 0.091728\n",
      "Epoch 255/20000, Training Loss: 0.092213, Validation Loss: 0.091311\n",
      "Epoch 256/20000, Training Loss: 0.091792, Validation Loss: 0.090897\n",
      "Epoch 257/20000, Training Loss: 0.091376, Validation Loss: 0.090486\n",
      "Epoch 258/20000, Training Loss: 0.090962, Validation Loss: 0.090078\n",
      "Epoch 259/20000, Training Loss: 0.090551, Validation Loss: 0.089672\n",
      "Epoch 260/20000, Training Loss: 0.090143, Validation Loss: 0.089270\n",
      "Epoch 261/20000, Training Loss: 0.089738, Validation Loss: 0.088870\n",
      "Epoch 262/20000, Training Loss: 0.089336, Validation Loss: 0.088473\n",
      "Epoch 263/20000, Training Loss: 0.088937, Validation Loss: 0.088079\n",
      "Epoch 264/20000, Training Loss: 0.088540, Validation Loss: 0.087687\n",
      "Epoch 265/20000, Training Loss: 0.088146, Validation Loss: 0.087298\n",
      "Epoch 266/20000, Training Loss: 0.087754, Validation Loss: 0.086912\n",
      "Epoch 267/20000, Training Loss: 0.087365, Validation Loss: 0.086528\n",
      "Epoch 268/20000, Training Loss: 0.086979, Validation Loss: 0.086146\n",
      "Epoch 269/20000, Training Loss: 0.086595, Validation Loss: 0.085767\n",
      "Epoch 270/20000, Training Loss: 0.086213, Validation Loss: 0.085390\n",
      "Epoch 271/20000, Training Loss: 0.085834, Validation Loss: 0.085016\n",
      "Epoch 272/20000, Training Loss: 0.085457, Validation Loss: 0.084644\n",
      "Epoch 273/20000, Training Loss: 0.085083, Validation Loss: 0.084274\n",
      "Epoch 274/20000, Training Loss: 0.084710, Validation Loss: 0.083906\n",
      "Epoch 275/20000, Training Loss: 0.084340, Validation Loss: 0.083541\n",
      "Epoch 276/20000, Training Loss: 0.083972, Validation Loss: 0.083177\n",
      "Epoch 277/20000, Training Loss: 0.083607, Validation Loss: 0.082816\n",
      "Epoch 278/20000, Training Loss: 0.083243, Validation Loss: 0.082457\n",
      "Epoch 279/20000, Training Loss: 0.082881, Validation Loss: 0.082099\n",
      "Epoch 280/20000, Training Loss: 0.082522, Validation Loss: 0.081744\n",
      "Epoch 281/20000, Training Loss: 0.082164, Validation Loss: 0.081391\n",
      "Epoch 282/20000, Training Loss: 0.081808, Validation Loss: 0.081040\n",
      "Epoch 283/20000, Training Loss: 0.081455, Validation Loss: 0.080690\n",
      "Epoch 284/20000, Training Loss: 0.081103, Validation Loss: 0.080343\n",
      "Epoch 285/20000, Training Loss: 0.080753, Validation Loss: 0.079997\n",
      "Epoch 286/20000, Training Loss: 0.080405, Validation Loss: 0.079654\n",
      "Epoch 287/20000, Training Loss: 0.080059, Validation Loss: 0.079312\n",
      "Epoch 288/20000, Training Loss: 0.079715, Validation Loss: 0.078972\n",
      "Epoch 289/20000, Training Loss: 0.079373, Validation Loss: 0.078634\n",
      "Epoch 290/20000, Training Loss: 0.079032, Validation Loss: 0.078298\n",
      "Epoch 291/20000, Training Loss: 0.078693, Validation Loss: 0.077963\n",
      "Epoch 292/20000, Training Loss: 0.078356, Validation Loss: 0.077630\n",
      "Epoch 293/20000, Training Loss: 0.078021, Validation Loss: 0.077299\n",
      "Epoch 294/20000, Training Loss: 0.077687, Validation Loss: 0.076969\n",
      "Epoch 295/20000, Training Loss: 0.077356, Validation Loss: 0.076642\n",
      "Epoch 296/20000, Training Loss: 0.077025, Validation Loss: 0.076315\n",
      "Epoch 297/20000, Training Loss: 0.076697, Validation Loss: 0.075991\n",
      "Epoch 298/20000, Training Loss: 0.076370, Validation Loss: 0.075668\n",
      "Epoch 299/20000, Training Loss: 0.076045, Validation Loss: 0.075347\n",
      "Epoch 300/20000, Training Loss: 0.075721, Validation Loss: 0.075027\n",
      "Epoch 301/20000, Training Loss: 0.075399, Validation Loss: 0.074709\n",
      "Epoch 302/20000, Training Loss: 0.075079, Validation Loss: 0.074393\n",
      "Epoch 303/20000, Training Loss: 0.074760, Validation Loss: 0.074078\n",
      "Epoch 304/20000, Training Loss: 0.074443, Validation Loss: 0.073765\n",
      "Epoch 305/20000, Training Loss: 0.074127, Validation Loss: 0.073453\n",
      "Epoch 306/20000, Training Loss: 0.073813, Validation Loss: 0.073143\n",
      "Epoch 307/20000, Training Loss: 0.073500, Validation Loss: 0.072834\n",
      "Epoch 308/20000, Training Loss: 0.073189, Validation Loss: 0.072527\n",
      "Epoch 309/20000, Training Loss: 0.072880, Validation Loss: 0.072221\n",
      "Epoch 310/20000, Training Loss: 0.072571, Validation Loss: 0.071917\n",
      "Epoch 311/20000, Training Loss: 0.072265, Validation Loss: 0.071614\n",
      "Epoch 312/20000, Training Loss: 0.071959, Validation Loss: 0.071313\n",
      "Epoch 313/20000, Training Loss: 0.071656, Validation Loss: 0.071013\n",
      "Epoch 314/20000, Training Loss: 0.071353, Validation Loss: 0.070714\n",
      "Epoch 315/20000, Training Loss: 0.071053, Validation Loss: 0.070417\n",
      "Epoch 316/20000, Training Loss: 0.070753, Validation Loss: 0.070122\n",
      "Epoch 317/20000, Training Loss: 0.070455, Validation Loss: 0.069827\n",
      "Epoch 318/20000, Training Loss: 0.070159, Validation Loss: 0.069534\n",
      "Epoch 319/20000, Training Loss: 0.069863, Validation Loss: 0.069243\n",
      "Epoch 320/20000, Training Loss: 0.069570, Validation Loss: 0.068953\n",
      "Epoch 321/20000, Training Loss: 0.069277, Validation Loss: 0.068664\n",
      "Epoch 322/20000, Training Loss: 0.068986, Validation Loss: 0.068377\n",
      "Epoch 323/20000, Training Loss: 0.068696, Validation Loss: 0.068091\n",
      "Epoch 324/20000, Training Loss: 0.068408, Validation Loss: 0.067806\n",
      "Epoch 325/20000, Training Loss: 0.068121, Validation Loss: 0.067523\n",
      "Epoch 326/20000, Training Loss: 0.067835, Validation Loss: 0.067241\n",
      "Epoch 327/20000, Training Loss: 0.067551, Validation Loss: 0.066960\n",
      "Epoch 328/20000, Training Loss: 0.067268, Validation Loss: 0.066681\n",
      "Epoch 329/20000, Training Loss: 0.066986, Validation Loss: 0.066403\n",
      "Epoch 330/20000, Training Loss: 0.066705, Validation Loss: 0.066126\n",
      "Epoch 331/20000, Training Loss: 0.066426, Validation Loss: 0.065851\n",
      "Epoch 332/20000, Training Loss: 0.066148, Validation Loss: 0.065577\n",
      "Epoch 333/20000, Training Loss: 0.065872, Validation Loss: 0.065304\n",
      "Epoch 334/20000, Training Loss: 0.065596, Validation Loss: 0.065032\n",
      "Epoch 335/20000, Training Loss: 0.065322, Validation Loss: 0.064762\n",
      "Epoch 336/20000, Training Loss: 0.065049, Validation Loss: 0.064493\n",
      "Epoch 337/20000, Training Loss: 0.064778, Validation Loss: 0.064225\n",
      "Epoch 338/20000, Training Loss: 0.064507, Validation Loss: 0.063958\n",
      "Epoch 339/20000, Training Loss: 0.064238, Validation Loss: 0.063693\n",
      "Epoch 340/20000, Training Loss: 0.063970, Validation Loss: 0.063428\n",
      "Epoch 341/20000, Training Loss: 0.063704, Validation Loss: 0.063165\n",
      "Epoch 342/20000, Training Loss: 0.063438, Validation Loss: 0.062903\n",
      "Epoch 343/20000, Training Loss: 0.063174, Validation Loss: 0.062643\n",
      "Epoch 344/20000, Training Loss: 0.062911, Validation Loss: 0.062383\n",
      "Epoch 345/20000, Training Loss: 0.062649, Validation Loss: 0.062125\n",
      "Epoch 346/20000, Training Loss: 0.062388, Validation Loss: 0.061868\n",
      "Epoch 347/20000, Training Loss: 0.062129, Validation Loss: 0.061612\n",
      "Epoch 348/20000, Training Loss: 0.061871, Validation Loss: 0.061357\n",
      "Epoch 349/20000, Training Loss: 0.061614, Validation Loss: 0.061104\n",
      "Epoch 350/20000, Training Loss: 0.061358, Validation Loss: 0.060851\n",
      "Epoch 351/20000, Training Loss: 0.061103, Validation Loss: 0.060600\n",
      "Epoch 352/20000, Training Loss: 0.060850, Validation Loss: 0.060350\n",
      "Epoch 353/20000, Training Loss: 0.060597, Validation Loss: 0.060101\n",
      "Epoch 354/20000, Training Loss: 0.060346, Validation Loss: 0.059854\n",
      "Epoch 355/20000, Training Loss: 0.060096, Validation Loss: 0.059607\n",
      "Epoch 356/20000, Training Loss: 0.059847, Validation Loss: 0.059362\n",
      "Epoch 357/20000, Training Loss: 0.059600, Validation Loss: 0.059117\n",
      "Epoch 358/20000, Training Loss: 0.059353, Validation Loss: 0.058874\n",
      "Epoch 359/20000, Training Loss: 0.059108, Validation Loss: 0.058632\n",
      "Epoch 360/20000, Training Loss: 0.058863, Validation Loss: 0.058391\n",
      "Epoch 361/20000, Training Loss: 0.058620, Validation Loss: 0.058151\n",
      "Epoch 362/20000, Training Loss: 0.058378, Validation Loss: 0.057913\n",
      "Epoch 363/20000, Training Loss: 0.058137, Validation Loss: 0.057675\n",
      "Epoch 364/20000, Training Loss: 0.057897, Validation Loss: 0.057439\n",
      "Epoch 365/20000, Training Loss: 0.057659, Validation Loss: 0.057203\n",
      "Epoch 366/20000, Training Loss: 0.057421, Validation Loss: 0.056969\n",
      "Epoch 367/20000, Training Loss: 0.057184, Validation Loss: 0.056736\n",
      "Epoch 368/20000, Training Loss: 0.056949, Validation Loss: 0.056504\n",
      "Epoch 369/20000, Training Loss: 0.056714, Validation Loss: 0.056273\n",
      "Epoch 370/20000, Training Loss: 0.056481, Validation Loss: 0.056043\n",
      "Epoch 371/20000, Training Loss: 0.056249, Validation Loss: 0.055814\n",
      "Epoch 372/20000, Training Loss: 0.056018, Validation Loss: 0.055586\n",
      "Epoch 373/20000, Training Loss: 0.055788, Validation Loss: 0.055359\n",
      "Epoch 374/20000, Training Loss: 0.055558, Validation Loss: 0.055133\n",
      "Epoch 375/20000, Training Loss: 0.055330, Validation Loss: 0.054909\n",
      "Epoch 376/20000, Training Loss: 0.055104, Validation Loss: 0.054685\n",
      "Epoch 377/20000, Training Loss: 0.054878, Validation Loss: 0.054462\n",
      "Epoch 378/20000, Training Loss: 0.054653, Validation Loss: 0.054241\n",
      "Epoch 379/20000, Training Loss: 0.054429, Validation Loss: 0.054020\n",
      "Epoch 380/20000, Training Loss: 0.054207, Validation Loss: 0.053801\n",
      "Epoch 381/20000, Training Loss: 0.053985, Validation Loss: 0.053582\n",
      "Epoch 382/20000, Training Loss: 0.053765, Validation Loss: 0.053365\n",
      "Epoch 383/20000, Training Loss: 0.053545, Validation Loss: 0.053148\n",
      "Epoch 384/20000, Training Loss: 0.053327, Validation Loss: 0.052933\n",
      "Epoch 385/20000, Training Loss: 0.053110, Validation Loss: 0.052719\n",
      "Epoch 386/20000, Training Loss: 0.052893, Validation Loss: 0.052505\n",
      "Epoch 387/20000, Training Loss: 0.052678, Validation Loss: 0.052293\n",
      "Epoch 388/20000, Training Loss: 0.052463, Validation Loss: 0.052082\n",
      "Epoch 389/20000, Training Loss: 0.052250, Validation Loss: 0.051871\n",
      "Epoch 390/20000, Training Loss: 0.052038, Validation Loss: 0.051662\n",
      "Epoch 391/20000, Training Loss: 0.051826, Validation Loss: 0.051454\n",
      "Epoch 392/20000, Training Loss: 0.051616, Validation Loss: 0.051247\n",
      "Epoch 393/20000, Training Loss: 0.051406, Validation Loss: 0.051040\n",
      "Epoch 394/20000, Training Loss: 0.051198, Validation Loss: 0.050835\n",
      "Epoch 395/20000, Training Loss: 0.050991, Validation Loss: 0.050630\n",
      "Epoch 396/20000, Training Loss: 0.050784, Validation Loss: 0.050427\n",
      "Epoch 397/20000, Training Loss: 0.050579, Validation Loss: 0.050224\n",
      "Epoch 398/20000, Training Loss: 0.050374, Validation Loss: 0.050022\n",
      "Epoch 399/20000, Training Loss: 0.050170, Validation Loss: 0.049822\n",
      "Epoch 400/20000, Training Loss: 0.049968, Validation Loss: 0.049622\n",
      "Epoch 401/20000, Training Loss: 0.049766, Validation Loss: 0.049423\n",
      "Epoch 402/20000, Training Loss: 0.049566, Validation Loss: 0.049226\n",
      "Epoch 403/20000, Training Loss: 0.049366, Validation Loss: 0.049029\n",
      "Epoch 404/20000, Training Loss: 0.049167, Validation Loss: 0.048833\n",
      "Epoch 405/20000, Training Loss: 0.048969, Validation Loss: 0.048638\n",
      "Epoch 406/20000, Training Loss: 0.048772, Validation Loss: 0.048444\n",
      "Epoch 407/20000, Training Loss: 0.048576, Validation Loss: 0.048251\n",
      "Epoch 408/20000, Training Loss: 0.048381, Validation Loss: 0.048058\n",
      "Epoch 409/20000, Training Loss: 0.048187, Validation Loss: 0.047867\n",
      "Epoch 410/20000, Training Loss: 0.047994, Validation Loss: 0.047677\n",
      "Epoch 411/20000, Training Loss: 0.047802, Validation Loss: 0.047487\n",
      "Epoch 412/20000, Training Loss: 0.047610, Validation Loss: 0.047299\n",
      "Epoch 413/20000, Training Loss: 0.047420, Validation Loss: 0.047111\n",
      "Epoch 414/20000, Training Loss: 0.047230, Validation Loss: 0.046924\n",
      "Epoch 415/20000, Training Loss: 0.047042, Validation Loss: 0.046739\n",
      "Epoch 416/20000, Training Loss: 0.046854, Validation Loss: 0.046554\n",
      "Epoch 417/20000, Training Loss: 0.046667, Validation Loss: 0.046370\n",
      "Epoch 418/20000, Training Loss: 0.046481, Validation Loss: 0.046187\n",
      "Epoch 419/20000, Training Loss: 0.046296, Validation Loss: 0.046004\n",
      "Epoch 420/20000, Training Loss: 0.046112, Validation Loss: 0.045823\n",
      "Epoch 421/20000, Training Loss: 0.045929, Validation Loss: 0.045643\n",
      "Epoch 422/20000, Training Loss: 0.045746, Validation Loss: 0.045463\n",
      "Epoch 423/20000, Training Loss: 0.045565, Validation Loss: 0.045284\n",
      "Epoch 424/20000, Training Loss: 0.045384, Validation Loss: 0.045106\n",
      "Epoch 425/20000, Training Loss: 0.045204, Validation Loss: 0.044929\n",
      "Epoch 426/20000, Training Loss: 0.045025, Validation Loss: 0.044753\n",
      "Epoch 427/20000, Training Loss: 0.044847, Validation Loss: 0.044578\n",
      "Epoch 428/20000, Training Loss: 0.044670, Validation Loss: 0.044404\n",
      "Epoch 429/20000, Training Loss: 0.044494, Validation Loss: 0.044230\n",
      "Epoch 430/20000, Training Loss: 0.044318, Validation Loss: 0.044058\n",
      "Epoch 431/20000, Training Loss: 0.044144, Validation Loss: 0.043886\n",
      "Epoch 432/20000, Training Loss: 0.043970, Validation Loss: 0.043715\n",
      "Epoch 433/20000, Training Loss: 0.043797, Validation Loss: 0.043544\n",
      "Epoch 434/20000, Training Loss: 0.043625, Validation Loss: 0.043375\n",
      "Epoch 435/20000, Training Loss: 0.043454, Validation Loss: 0.043206\n",
      "Epoch 436/20000, Training Loss: 0.043284, Validation Loss: 0.043039\n",
      "Epoch 437/20000, Training Loss: 0.043114, Validation Loss: 0.042872\n",
      "Epoch 438/20000, Training Loss: 0.042945, Validation Loss: 0.042706\n",
      "Epoch 439/20000, Training Loss: 0.042777, Validation Loss: 0.042541\n",
      "Epoch 440/20000, Training Loss: 0.042610, Validation Loss: 0.042376\n",
      "Epoch 441/20000, Training Loss: 0.042444, Validation Loss: 0.042213\n",
      "Epoch 442/20000, Training Loss: 0.042279, Validation Loss: 0.042050\n",
      "Epoch 443/20000, Training Loss: 0.042114, Validation Loss: 0.041888\n",
      "Epoch 444/20000, Training Loss: 0.041950, Validation Loss: 0.041727\n",
      "Epoch 445/20000, Training Loss: 0.041787, Validation Loss: 0.041566\n",
      "Epoch 446/20000, Training Loss: 0.041625, Validation Loss: 0.041407\n",
      "Epoch 447/20000, Training Loss: 0.041464, Validation Loss: 0.041248\n",
      "Epoch 448/20000, Training Loss: 0.041303, Validation Loss: 0.041090\n",
      "Epoch 449/20000, Training Loss: 0.041144, Validation Loss: 0.040933\n",
      "Epoch 450/20000, Training Loss: 0.040985, Validation Loss: 0.040776\n",
      "Epoch 451/20000, Training Loss: 0.040826, Validation Loss: 0.040620\n",
      "Epoch 452/20000, Training Loss: 0.040669, Validation Loss: 0.040466\n",
      "Epoch 453/20000, Training Loss: 0.040512, Validation Loss: 0.040311\n",
      "Epoch 454/20000, Training Loss: 0.040357, Validation Loss: 0.040158\n",
      "Epoch 455/20000, Training Loss: 0.040201, Validation Loss: 0.040005\n",
      "Epoch 456/20000, Training Loss: 0.040047, Validation Loss: 0.039854\n",
      "Epoch 457/20000, Training Loss: 0.039894, Validation Loss: 0.039702\n",
      "Epoch 458/20000, Training Loss: 0.039741, Validation Loss: 0.039552\n",
      "Epoch 459/20000, Training Loss: 0.039589, Validation Loss: 0.039403\n",
      "Epoch 460/20000, Training Loss: 0.039438, Validation Loss: 0.039254\n",
      "Epoch 461/20000, Training Loss: 0.039287, Validation Loss: 0.039106\n",
      "Epoch 462/20000, Training Loss: 0.039137, Validation Loss: 0.038959\n",
      "Epoch 463/20000, Training Loss: 0.038988, Validation Loss: 0.038812\n",
      "Epoch 464/20000, Training Loss: 0.038840, Validation Loss: 0.038666\n",
      "Epoch 465/20000, Training Loss: 0.038693, Validation Loss: 0.038521\n",
      "Epoch 466/20000, Training Loss: 0.038546, Validation Loss: 0.038377\n",
      "Epoch 467/20000, Training Loss: 0.038400, Validation Loss: 0.038234\n",
      "Epoch 468/20000, Training Loss: 0.038255, Validation Loss: 0.038091\n",
      "Epoch 469/20000, Training Loss: 0.038110, Validation Loss: 0.037949\n",
      "Epoch 470/20000, Training Loss: 0.037967, Validation Loss: 0.037807\n",
      "Epoch 471/20000, Training Loss: 0.037824, Validation Loss: 0.037667\n",
      "Epoch 472/20000, Training Loss: 0.037681, Validation Loss: 0.037527\n",
      "Epoch 473/20000, Training Loss: 0.037540, Validation Loss: 0.037388\n",
      "Epoch 474/20000, Training Loss: 0.037399, Validation Loss: 0.037250\n",
      "Epoch 475/20000, Training Loss: 0.037259, Validation Loss: 0.037112\n",
      "Epoch 476/20000, Training Loss: 0.037120, Validation Loss: 0.036975\n",
      "Epoch 477/20000, Training Loss: 0.036981, Validation Loss: 0.036839\n",
      "Epoch 478/20000, Training Loss: 0.036843, Validation Loss: 0.036703\n",
      "Epoch 479/20000, Training Loss: 0.036706, Validation Loss: 0.036568\n",
      "Epoch 480/20000, Training Loss: 0.036570, Validation Loss: 0.036434\n",
      "Epoch 481/20000, Training Loss: 0.036434, Validation Loss: 0.036301\n",
      "Epoch 482/20000, Training Loss: 0.036299, Validation Loss: 0.036168\n",
      "Epoch 483/20000, Training Loss: 0.036164, Validation Loss: 0.036036\n",
      "Epoch 484/20000, Training Loss: 0.036031, Validation Loss: 0.035905\n",
      "Epoch 485/20000, Training Loss: 0.035898, Validation Loss: 0.035774\n",
      "Epoch 486/20000, Training Loss: 0.035766, Validation Loss: 0.035644\n",
      "Epoch 487/20000, Training Loss: 0.035634, Validation Loss: 0.035515\n",
      "Epoch 488/20000, Training Loss: 0.035503, Validation Loss: 0.035386\n",
      "Epoch 489/20000, Training Loss: 0.035373, Validation Loss: 0.035258\n",
      "Epoch 490/20000, Training Loss: 0.035244, Validation Loss: 0.035131\n",
      "Epoch 491/20000, Training Loss: 0.035115, Validation Loss: 0.035004\n",
      "Epoch 492/20000, Training Loss: 0.034987, Validation Loss: 0.034878\n",
      "Epoch 493/20000, Training Loss: 0.034859, Validation Loss: 0.034753\n",
      "Epoch 494/20000, Training Loss: 0.034732, Validation Loss: 0.034629\n",
      "Epoch 495/20000, Training Loss: 0.034606, Validation Loss: 0.034505\n",
      "Epoch 496/20000, Training Loss: 0.034481, Validation Loss: 0.034382\n",
      "Epoch 497/20000, Training Loss: 0.034356, Validation Loss: 0.034259\n",
      "Epoch 498/20000, Training Loss: 0.034232, Validation Loss: 0.034137\n",
      "Epoch 499/20000, Training Loss: 0.034109, Validation Loss: 0.034016\n",
      "Epoch 500/20000, Training Loss: 0.033986, Validation Loss: 0.033895\n",
      "Epoch 501/20000, Training Loss: 0.033864, Validation Loss: 0.033775\n",
      "Epoch 502/20000, Training Loss: 0.033743, Validation Loss: 0.033656\n",
      "Epoch 503/20000, Training Loss: 0.033622, Validation Loss: 0.033537\n",
      "Epoch 504/20000, Training Loss: 0.033502, Validation Loss: 0.033419\n",
      "Epoch 505/20000, Training Loss: 0.033383, Validation Loss: 0.033302\n",
      "Epoch 506/20000, Training Loss: 0.033264, Validation Loss: 0.033185\n",
      "Epoch 507/20000, Training Loss: 0.033146, Validation Loss: 0.033069\n",
      "Epoch 508/20000, Training Loss: 0.033028, Validation Loss: 0.032954\n",
      "Epoch 509/20000, Training Loss: 0.032911, Validation Loss: 0.032839\n",
      "Epoch 510/20000, Training Loss: 0.032795, Validation Loss: 0.032724\n",
      "Epoch 511/20000, Training Loss: 0.032680, Validation Loss: 0.032611\n",
      "Epoch 512/20000, Training Loss: 0.032565, Validation Loss: 0.032498\n",
      "Epoch 513/20000, Training Loss: 0.032450, Validation Loss: 0.032386\n",
      "Epoch 514/20000, Training Loss: 0.032337, Validation Loss: 0.032274\n",
      "Epoch 515/20000, Training Loss: 0.032224, Validation Loss: 0.032163\n",
      "Epoch 516/20000, Training Loss: 0.032111, Validation Loss: 0.032052\n",
      "Epoch 517/20000, Training Loss: 0.032000, Validation Loss: 0.031942\n",
      "Epoch 518/20000, Training Loss: 0.031888, Validation Loss: 0.031833\n",
      "Epoch 519/20000, Training Loss: 0.031778, Validation Loss: 0.031724\n",
      "Epoch 520/20000, Training Loss: 0.031668, Validation Loss: 0.031616\n",
      "Epoch 521/20000, Training Loss: 0.031558, Validation Loss: 0.031508\n",
      "Epoch 522/20000, Training Loss: 0.031450, Validation Loss: 0.031401\n",
      "Epoch 523/20000, Training Loss: 0.031341, Validation Loss: 0.031295\n",
      "Epoch 524/20000, Training Loss: 0.031234, Validation Loss: 0.031189\n",
      "Epoch 525/20000, Training Loss: 0.031127, Validation Loss: 0.031084\n",
      "Epoch 526/20000, Training Loss: 0.031020, Validation Loss: 0.030979\n",
      "Epoch 527/20000, Training Loss: 0.030915, Validation Loss: 0.030875\n",
      "Epoch 528/20000, Training Loss: 0.030809, Validation Loss: 0.030772\n",
      "Epoch 529/20000, Training Loss: 0.030705, Validation Loss: 0.030669\n",
      "Epoch 530/20000, Training Loss: 0.030601, Validation Loss: 0.030566\n",
      "Epoch 531/20000, Training Loss: 0.030497, Validation Loss: 0.030465\n",
      "Epoch 532/20000, Training Loss: 0.030394, Validation Loss: 0.030363\n",
      "Epoch 533/20000, Training Loss: 0.030292, Validation Loss: 0.030263\n",
      "Epoch 534/20000, Training Loss: 0.030190, Validation Loss: 0.030163\n",
      "Epoch 535/20000, Training Loss: 0.030089, Validation Loss: 0.030063\n",
      "Epoch 536/20000, Training Loss: 0.029988, Validation Loss: 0.029964\n",
      "Epoch 537/20000, Training Loss: 0.029888, Validation Loss: 0.029866\n",
      "Epoch 538/20000, Training Loss: 0.029789, Validation Loss: 0.029768\n",
      "Epoch 539/20000, Training Loss: 0.029690, Validation Loss: 0.029671\n",
      "Epoch 540/20000, Training Loss: 0.029591, Validation Loss: 0.029574\n",
      "Epoch 541/20000, Training Loss: 0.029493, Validation Loss: 0.029478\n",
      "Epoch 542/20000, Training Loss: 0.029396, Validation Loss: 0.029382\n",
      "Epoch 543/20000, Training Loss: 0.029299, Validation Loss: 0.029287\n",
      "Epoch 544/20000, Training Loss: 0.029203, Validation Loss: 0.029193\n",
      "Epoch 545/20000, Training Loss: 0.029107, Validation Loss: 0.029099\n",
      "Epoch 546/20000, Training Loss: 0.029012, Validation Loss: 0.029005\n",
      "Epoch 547/20000, Training Loss: 0.028918, Validation Loss: 0.028912\n",
      "Epoch 548/20000, Training Loss: 0.028824, Validation Loss: 0.028820\n",
      "Epoch 549/20000, Training Loss: 0.028730, Validation Loss: 0.028728\n",
      "Epoch 550/20000, Training Loss: 0.028637, Validation Loss: 0.028637\n",
      "Epoch 551/20000, Training Loss: 0.028545, Validation Loss: 0.028546\n",
      "Epoch 552/20000, Training Loss: 0.028453, Validation Loss: 0.028455\n",
      "Epoch 553/20000, Training Loss: 0.028361, Validation Loss: 0.028366\n",
      "Epoch 554/20000, Training Loss: 0.028271, Validation Loss: 0.028276\n",
      "Epoch 555/20000, Training Loss: 0.028180, Validation Loss: 0.028187\n",
      "Epoch 556/20000, Training Loss: 0.028090, Validation Loss: 0.028099\n",
      "Epoch 557/20000, Training Loss: 0.028001, Validation Loss: 0.028011\n",
      "Epoch 558/20000, Training Loss: 0.027912, Validation Loss: 0.027924\n",
      "Epoch 559/20000, Training Loss: 0.027824, Validation Loss: 0.027837\n",
      "Epoch 560/20000, Training Loss: 0.027736, Validation Loss: 0.027751\n",
      "Epoch 561/20000, Training Loss: 0.027649, Validation Loss: 0.027665\n",
      "Epoch 562/20000, Training Loss: 0.027562, Validation Loss: 0.027580\n",
      "Epoch 563/20000, Training Loss: 0.027476, Validation Loss: 0.027495\n",
      "Epoch 564/20000, Training Loss: 0.027390, Validation Loss: 0.027411\n",
      "Epoch 565/20000, Training Loss: 0.027305, Validation Loss: 0.027327\n",
      "Epoch 566/20000, Training Loss: 0.027220, Validation Loss: 0.027243\n",
      "Epoch 567/20000, Training Loss: 0.027136, Validation Loss: 0.027160\n",
      "Epoch 568/20000, Training Loss: 0.027052, Validation Loss: 0.027078\n",
      "Epoch 569/20000, Training Loss: 0.026969, Validation Loss: 0.026996\n",
      "Epoch 570/20000, Training Loss: 0.026886, Validation Loss: 0.026915\n",
      "Epoch 571/20000, Training Loss: 0.026804, Validation Loss: 0.026834\n",
      "Epoch 572/20000, Training Loss: 0.026722, Validation Loss: 0.026753\n",
      "Epoch 573/20000, Training Loss: 0.026640, Validation Loss: 0.026673\n",
      "Epoch 574/20000, Training Loss: 0.026559, Validation Loss: 0.026593\n",
      "Epoch 575/20000, Training Loss: 0.026479, Validation Loss: 0.026514\n",
      "Epoch 576/20000, Training Loss: 0.026399, Validation Loss: 0.026435\n",
      "Epoch 577/20000, Training Loss: 0.026319, Validation Loss: 0.026357\n",
      "Epoch 578/20000, Training Loss: 0.026240, Validation Loss: 0.026279\n",
      "Epoch 579/20000, Training Loss: 0.026162, Validation Loss: 0.026202\n",
      "Epoch 580/20000, Training Loss: 0.026084, Validation Loss: 0.026125\n",
      "Epoch 581/20000, Training Loss: 0.026006, Validation Loss: 0.026048\n",
      "Epoch 582/20000, Training Loss: 0.025929, Validation Loss: 0.025972\n",
      "Epoch 583/20000, Training Loss: 0.025852, Validation Loss: 0.025897\n",
      "Epoch 584/20000, Training Loss: 0.025775, Validation Loss: 0.025822\n",
      "Epoch 585/20000, Training Loss: 0.025699, Validation Loss: 0.025747\n",
      "Epoch 586/20000, Training Loss: 0.025624, Validation Loss: 0.025673\n",
      "Epoch 587/20000, Training Loss: 0.025549, Validation Loss: 0.025599\n",
      "Epoch 588/20000, Training Loss: 0.025474, Validation Loss: 0.025525\n",
      "Epoch 589/20000, Training Loss: 0.025400, Validation Loss: 0.025452\n",
      "Epoch 590/20000, Training Loss: 0.025326, Validation Loss: 0.025380\n",
      "Epoch 591/20000, Training Loss: 0.025253, Validation Loss: 0.025308\n",
      "Epoch 592/20000, Training Loss: 0.025180, Validation Loss: 0.025236\n",
      "Epoch 593/20000, Training Loss: 0.025108, Validation Loss: 0.025165\n",
      "Epoch 594/20000, Training Loss: 0.025036, Validation Loss: 0.025094\n",
      "Epoch 595/20000, Training Loss: 0.024964, Validation Loss: 0.025023\n",
      "Epoch 596/20000, Training Loss: 0.024893, Validation Loss: 0.024953\n",
      "Epoch 597/20000, Training Loss: 0.024822, Validation Loss: 0.024883\n",
      "Epoch 598/20000, Training Loss: 0.024752, Validation Loss: 0.024814\n",
      "Epoch 599/20000, Training Loss: 0.024682, Validation Loss: 0.024745\n",
      "Epoch 600/20000, Training Loss: 0.024612, Validation Loss: 0.024677\n",
      "Epoch 601/20000, Training Loss: 0.024543, Validation Loss: 0.024609\n",
      "Epoch 602/20000, Training Loss: 0.024474, Validation Loss: 0.024541\n",
      "Epoch 603/20000, Training Loss: 0.024406, Validation Loss: 0.024474\n",
      "Epoch 604/20000, Training Loss: 0.024338, Validation Loss: 0.024407\n",
      "Epoch 605/20000, Training Loss: 0.024270, Validation Loss: 0.024340\n",
      "Epoch 606/20000, Training Loss: 0.024203, Validation Loss: 0.024274\n",
      "Epoch 607/20000, Training Loss: 0.024136, Validation Loss: 0.024208\n",
      "Epoch 608/20000, Training Loss: 0.024070, Validation Loss: 0.024143\n",
      "Epoch 609/20000, Training Loss: 0.024004, Validation Loss: 0.024077\n",
      "Epoch 610/20000, Training Loss: 0.023938, Validation Loss: 0.024013\n",
      "Epoch 611/20000, Training Loss: 0.023873, Validation Loss: 0.023948\n",
      "Epoch 612/20000, Training Loss: 0.023808, Validation Loss: 0.023884\n",
      "Epoch 613/20000, Training Loss: 0.023743, Validation Loss: 0.023821\n",
      "Epoch 614/20000, Training Loss: 0.023679, Validation Loss: 0.023757\n",
      "Epoch 615/20000, Training Loss: 0.023615, Validation Loss: 0.023694\n",
      "Epoch 616/20000, Training Loss: 0.023552, Validation Loss: 0.023632\n"
     ]
    }
   ],
   "source": [
    "# Kolmogorov-Arnold Neural Network Implementation in PyTorch\n",
    "class KolmogorovArnoldNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_summation_terms):\n",
    "        \"\"\"\n",
    "        Initialize the Kolmogorov-Arnold Neural Network.\n",
    "        Args:\n",
    "            input_dim: Number of input variables (dimensionality of the input).\n",
    "            hidden_dim: Number of hidden neurons in each univariate sub-network.\n",
    "            output_dim: Number of outputs (dimensionality of the output).\n",
    "            num_summation_terms: Number of summation terms in the decomposition.\n",
    "        \"\"\"\n",
    "        super(KolmogorovArnoldNet, self).__init__()\n",
    "\n",
    "        # Univariate networks: One for each summation term\n",
    "        self.univariate_nets = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),  # Fully connected layer\n",
    "                nn.ReLU(),                        # Activation function\n",
    "                nn.Linear(hidden_dim, 1)          # Map to a scalar output\n",
    "            )\n",
    "            for _ in range(num_summation_terms)\n",
    "        ])\n",
    "\n",
    "        # Output weights for combining the summation terms\n",
    "        self.output_weights = nn.Parameter(torch.randn(num_summation_terms, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim).\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        # Compute outputs from each univariate network\n",
    "        summation_terms = torch.cat([net(x) for net in self.univariate_nets], dim=1)\n",
    "\n",
    "        # Linearly combine summation terms using output weights\n",
    "        output = summation_terms @ self.output_weights\n",
    "        return output\n",
    "\n",
    "\n",
    "# Define the synthetic target function (for demonstration purposes)\n",
    "def target_function(x):\n",
    "    \"\"\"\n",
    "    Target multivariate function to approximate.\n",
    "    Args:\n",
    "        x: Input tensor of shape (batch_size, input_dim).\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size, 1) with function values.\n",
    "    \"\"\"\n",
    "    # Example: A nonlinear function of multiple variables\n",
    "    return torch.sin(x[:, 0]*x[:, 1]) + (torch.cos(x[:, 1])**2)*torch.sin(x[:,2]) + 0.5 * x[:, 2]**2\n",
    "\n",
    "# Parameters of the net \n",
    "input_dim=3\n",
    "num_samples = 100000\n",
    "learning_rate = 0.0005\n",
    "epochs=20000\n",
    "\n",
    "# Generate synthetic data\n",
    "x = torch.rand(num_samples, input_dim) * 2 - 1  # Random values in [-1, 1]\n",
    "y = target_function(x).unsqueeze(1)  # Compute target values and add output dimension\n",
    "\n",
    "\n",
    "# Creating list of epochs\n",
    "L1 = list(range(1, epochs + 1))\n",
    "Val = []\n",
    "Val1 = []\n",
    "\n",
    "\n",
    "# Training the Kolmogorov-Arnold Neural Network\n",
    "def train_kann(x,y,num_samples):\n",
    "    \"\"\"\n",
    "    Train the Kolmogorov-Arnold Neural Network to approximate the target function.\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    #input_dim = 3          # Number of input variables\n",
    "    hidden_dim = 10        # Hidden layer size in each sub-network\n",
    "    output_dim = 1         # Number of outputs\n",
    "    num_summation_terms = 10  # Number of summation terms in the decomposition\n",
    "    batch_size = 32        # Training batch size\n",
    "    #epochs = 10000           # Number of training epochs\n",
    "    #learning_rate = 0.005   # Learning rate\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    train_size = int(0.5 * num_samples)\n",
    "    x_train, y_train = x[:train_size], y[:train_size]\n",
    "    x_val, y_val = x[train_size:], y[train_size:]\n",
    "\n",
    "    # Initialize the network\n",
    "    kann = KolmogorovArnoldNet(input_dim, hidden_dim, output_dim, num_summation_terms)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(kann.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training step\n",
    "        kann.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = kann(x_train)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        kann.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = kann(x_val)\n",
    "            val_loss = criterion(y_val_pred, y_val)\n",
    "            Val.append(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss.item():.6f}, Validation Loss: {val_loss.item():.6f}\")\n",
    "\n",
    "    # Return the trained network\n",
    "    return kann\n",
    "\n",
    "\n",
    "# Train the network\n",
    "if __name__ == \"__main__\":\n",
    "    trained_kann = train_kann(x,y,num_samples)\n",
    "\n",
    "    # Test on new data\n",
    "    test_data = torch.tensor([[0.1, -0.2, 0.3]])\n",
    "    with torch.no_grad():\n",
    "        prediction = trained_kann(test_data)\n",
    "    print(f\"Prediction for input {test_data.numpy()}: {prediction.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc4049-792b-4759-a9c0-ed8cb9972f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conventional Neural Network Implementation in PyTorch\n",
    "class ConventionalNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers):\n",
    "        \"\"\"\n",
    "        Initialize a conventional fully connected neural network.\n",
    "        Args:\n",
    "            input_dim: Number of input variables (dimensionality of the input).\n",
    "            hidden_dim: Number of neurons in each hidden layer.\n",
    "            output_dim: Number of outputs (dimensionality of the output).\n",
    "            num_hidden_layers: Number of hidden layers in the network.\n",
    "        \"\"\"\n",
    "        super(ConventionalNN, self).__init__()\n",
    "        \n",
    "        # Create the input layer\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Add the output layer\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        # Register the network layers\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim).\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# Define the synthetic target function (for demonstration purposes)\n",
    "def target_function(x):\n",
    "    \"\"\"\n",
    "    Target multivariate function to approximate.\n",
    "    Args:\n",
    "        x: Input tensor of shape (batch_size, input_dim).\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size, 1) with function values.\n",
    "    \"\"\"\n",
    "    # Example: A nonlinear function of multiple variables\n",
    "    return torch.sin(x[:, 0]*x[:, 1]) + (torch.cos(x[:, 1])**2)*torch.sin(x[:,2]) + 0.5 * x[:, 2]**2\n",
    "\n",
    "\n",
    "# Training the Conventional Neural Network\n",
    "def train_conventional_nn():\n",
    "    \"\"\"\n",
    "    Train a conventional fully connected neural network to approximate the target function.\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    #input_dim = 3          # Number of input variables\n",
    "    hidden_dim = 20        # Hidden layer size\n",
    "    output_dim = 1         # Number of outputs\n",
    "    num_hidden_layers = 3  # Number of hidden layers\n",
    "    batch_size = 32        # Training batch size\n",
    "    #epochs = 10000           # Number of training epochs\n",
    "    #learning_rate = 0.01   # Learning rate\n",
    "\n",
    "\n",
    "    # Split into training and validation sets\n",
    "    train_size = int(0.5 * num_samples)\n",
    "    x_train, y_train = x[:train_size], y[:train_size]\n",
    "    x_val, y_val = x[train_size:], y[train_size:]\n",
    "    \n",
    "    # Initialize the network\n",
    "    nn_model = ConventionalNN(input_dim, hidden_dim, output_dim, num_hidden_layers)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Training step\n",
    "        nn_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = nn_model(x_train)\n",
    "        train_loss = criterion(y_pred, y_train)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        nn_model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = nn_model(x_val)\n",
    "            val_loss = criterion(y_val_pred, y_val)\n",
    "            Val1.append(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss.item():.6f}, Validation Loss: {val_loss.item():.6f}\")\n",
    "\n",
    "    # Return the trained network\n",
    "    return nn_model\n",
    "\n",
    "\n",
    "# Train the network\n",
    "if __name__ == \"__main__\":\n",
    "    trained_nn_model = train_conventional_nn()\n",
    "\n",
    "    # Test on new data\n",
    "    test_data = torch.tensor([[0.1, -0.2, 0.3]])\n",
    "    with torch.no_grad():\n",
    "        prediction = trained_nn_model(test_data)\n",
    "    print(f\"Prediction for input {test_data.numpy()}: {prediction.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4947f8d-8a75-438a-90e7-04dc7124ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(L1, Val, marker='.', linestyle='-', color='b', linewidth=0.5, markersize=0.5, label='KANN')\n",
    "plt.plot(L1, Val1, marker='.', linestyle='-', color='r', linewidth=0.5, markersize=0.5, label='Conv. NN')\n",
    "\n",
    "# Add labels and title\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Validation error')\n",
    "plt.title('Graph of Validation error over epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Show the graph\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b887133-8e6c-4e0f-9e43-2c8a17b978c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
